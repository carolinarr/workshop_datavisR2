<!DOCTYPE html><html><head><title>R: Tokenize a file/string.</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R.css" />
</head><body><div class="container">

<table style="width: 100%;"><tr><td>tokenize {readr}</td><td style="text-align: right;">R Documentation</td></tr></table>

<h2 id='tokenize'>Tokenize a file/string.</h2>

<h3>Description</h3>

<p>Turns input into a character vector. Usually the tokenization is done purely
in C++, and never exposed to R (because that requires a copy). This function
is useful for testing, or when a file doesn't parse correctly and you want
to see the underlying tokens.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokenize(file, tokenizer = tokenizer_csv(), skip = 0, n_max = -1L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokenize_:_file">file</code></td>
<td>
<p>Either a path to a file, a connection, or literal data
(either a single string or a raw vector).
</p>
<p>Files ending in <code>.gz</code>, <code>.bz2</code>, <code>.xz</code>, or <code>.zip</code> will
be automatically uncompressed. Files starting with <code style="white-space: pre;">&#8288;http://&#8288;</code>,
<code style="white-space: pre;">&#8288;https://&#8288;</code>, <code style="white-space: pre;">&#8288;ftp://&#8288;</code>, or <code style="white-space: pre;">&#8288;ftps://&#8288;</code> will be automatically
downloaded. Remote gz files can also be automatically downloaded and
decompressed.
</p>
<p>Literal data is most useful for examples and tests. To be recognised as
literal data, the input must be either wrapped with <code>I()</code>, be a string
containing at least one new line, or be a vector containing at least one
string with a new line.
</p>
<p>Using a value of <code><a href="../../readr/help/clipboard.html">clipboard()</a></code> will read from the system clipboard.</p>
</td></tr>
<tr><td><code id="tokenize_:_tokenizer">tokenizer</code></td>
<td>
<p>A tokenizer specification.</p>
</td></tr>
<tr><td><code id="tokenize_:_skip">skip</code></td>
<td>
<p>Number of lines to skip before reading data.</p>
</td></tr>
<tr><td><code id="tokenize_:_n_max">n_max</code></td>
<td>
<p>Optionally, maximum number of rows to tokenize.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>tokenize("1,2\n3,4,5\n\n6")

# Only tokenize first two lines
tokenize("1,2\n3,4,5\n\n6", n = 2)
</code></pre>

<hr /><div style="text-align: center;">[Package <em>readr</em> version 2.1.4 <a href="00Index.html">Index</a>]</div>
</div>
</body></html>
